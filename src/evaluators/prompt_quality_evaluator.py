"""Prompt quality evaluator for meta-prompting and image prompts."""

import os
from typing import Dict, List, Any

from ..clients.base_client import BaseClient
from ..utils.config import load_evaluation_metrics

class PromptQualityEvaluator:
    """Evaluates quality of generated prompts for downstream use."""

    def __init__(self, evaluation_model: BaseClient, metrics_config: Dict[str, Any] = None):
        """
        Initialize the prompt quality evaluator.

        Args:
            evaluation_model: Model client for evaluation
            metrics_config: Metrics configuration dictionary
        """
        self.evaluation_model = evaluation_model

        # Load metrics configuration if not provided
        if metrics_config is None:
            config_dir = os.environ.get("CONFIG_DIR", "./config")
            self.metrics_config = load_evaluation_metrics(f"{config_dir}/evaluation/metrics.yaml")
        else:
            self.metrics_config = metrics_config

    def evaluate(self,
                 original_prompt: str,
                 generated_prompt: str,
                 prompt_type: str = "meta",  # "meta" or "image"
                 prompt_purpose: str = None,
                 target_system: str = None,
                 metrics: List[str] = None) -> Dict[str, Any]:
        """
        Evaluate the quality of a generated prompt.

        Args:
            original_prompt: Original prompt asking to generate a prompt
            generated_prompt: The prompt generated by the model
            prompt_type: Type of prompt ("meta" or "image")
            prompt_purpose: Purpose or goal of the prompt
            target_system: Target system for the prompt
            metrics: Specific metrics to evaluate

        Returns:
            Dictionary of evaluation scores
        """
        if not metrics:
            if prompt_type == "image":
                metrics = ["clarity", "specificity", "consistency"]
            else:  # meta
                metrics = ["clarity", "specificity", "effectiveness"]

        results = {}

        # If no evaluation model is available, provide mock scores
        if self.evaluation_model is None:
            for metric in metrics:
                results[metric] = 0.8  # Default moderate score
            
            results["overall_score"] = 0.8
            return results

        for metric in metrics:
            if metric not in self.metrics_config:
                continue

            metric_config = self.metrics_config.get(metric, {})

            if metric_config.get("evaluation_method") == "model_based":
                # Create evaluation prompt
                eval_prompt = self._create_evaluation_prompt(
                    original_prompt, generated_prompt, prompt_type,
                    prompt_purpose, target_system, metric
                )

                # Generate evaluation using evaluation model
                try:
                    eval_response = self.evaluation_model.generate(
                        prompt=eval_prompt,
                        config={"system_prompt": f"You are an expert evaluator assessing {prompt_type} prompt quality."}
                    )

                    # Parse score from response
                    score = self._parse_score(eval_response, metric_config.get("scale", [0, 5]))
                    results[metric] = score
                except Exception as e:
                    results[metric] = {
                        "score": 0,
                        "error": str(e)
                    }
            else:
                # Implement rule-based or other evaluation methods here
                results[metric] = 0.7  # Default fallback score

        # Calculate weighted average score
        weighted_score = 0
        total_weight = 0

        for metric, score in results.items():
            if isinstance(score, dict):
                # Skip if there was an error
                continue

            metric_weight = self.metrics_config.get(metric, {}).get("weight", 1.0)
            weighted_score += score * metric_weight
            total_weight += metric_weight

        if total_weight > 0:
            results["overall_score"] = weighted_score / total_weight
        else:
            results["overall_score"] = 0

        return results

    def _create_evaluation_prompt(self, original_prompt: str, generated_prompt: str,
                                  prompt_type: str, prompt_purpose: str, target_system: str,
                                  metric: str) -> str:
        """Create a prompt for evaluating prompt quality."""
        metric_config = self.metrics_config.get(metric, {})
        scale = metric_config.get("scale", [0, 5])
        scale_description = ", ".join([f"{i}: {desc}" for i, desc in enumerate(metric_config.get("scale_descriptions", []))])

        if prompt_type == "image":
            template = f"""
            Please evaluate the quality of the following image generation prompt for {metric_config.get("description", metric)}.

            Original request:
            "{original_prompt}"

            Generated image prompt:
            "{generated_prompt}"
            """

            if prompt_purpose:
                template += f"""
                Purpose of the image:
                "{prompt_purpose}"
                """

            template += f"""
            On a scale of {min(scale)} to {max(scale)},
            where {min(scale)} means poor image prompt and {max(scale)} means excellent image prompt,
            rate the prompt and explain your rating.

            Consider:
            1. Does the prompt clearly describe the desired image?
            2. Does the prompt include specific details about style, composition, and elements?
            3. Would an image generation model likely produce a consistent, high-quality image from this prompt?
            4. Is the prompt appropriate for the purpose?

            Your answer should be in this format:
            Rating: [numeric score between {min(scale)} and {max(scale)}]
            Explanation: [your explanation]
            Strengths: [list specific strengths of the prompt]
            Weaknesses: [list specific weaknesses of the prompt]
            Suggested improvements: [brief suggestions to improve the prompt]
            """
        else:  # meta prompt
            template = f"""
            Please evaluate the quality of the following meta-prompt for {metric_config.get("description", metric)}.

            Original request:
            "{original_prompt}"

            Generated meta-prompt:
            "{generated_prompt}"
            """

            if prompt_purpose:
                template += f"""
                Purpose of the prompt:
                "{prompt_purpose}"
                """

            if target_system:
                template += f"""
                Target system:
                "{target_system}"
                """

            template += f"""
            On a scale of {min(scale)} to {max(scale)},
            where {min(scale)} means poor meta-prompt and {max(scale)} means excellent meta-prompt,
            rate the prompt and explain your rating.

            Consider:
            1. Does the prompt clearly communicate the desired output?
            2. Does the prompt include specific instructions that guide the model effectively?
            3. Would the prompt likely elicit a high-quality response from an AI system?
            4. Is the prompt structured in a way that maximizes the chance of success?

            Your answer should be in this format:
            Rating: [numeric score between {min(scale)} and {max(scale)}]
            Explanation: [your explanation]
            Strengths: [list specific strengths of the prompt]
            Weaknesses: [list specific weaknesses of the prompt]
            Suggested improvements: [brief suggestions to improve the prompt]
            """

        return template

    def _parse_score(self, evaluation_text: str, scale: List[int]) -> float:
        """Parse the score from evaluation response."""
        try:
            # Look for "Rating: X" pattern
            for line in evaluation_text.split("\n"):
                if line.lower().startswith("rating:"):
                    score_str = line.split(":", 1)[1].strip()
                    # Extract first number found
                    import re
                    numbers = re.findall(r'\d+\.?\d*', score_str)
                    if numbers:
                        score = float(numbers[0])
                        # Ensure score is within scale
                        return max(min(score, max(scale)), min(scale))

            # If no rating found, assume middle score
            return (max(scale) + min(scale)) / 2
        except Exception:
            # Default to middle score on error
            return (max(scale) + min(scale)) / 2